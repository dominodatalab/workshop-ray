{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb37dda2-9b5e-4ecd-831b-c40bbc21713c",
   "metadata": {},
   "source": [
    "# Experimentation and troubleshooting for Dan\n",
    "\n",
    "Any important learnings should get documented/folded into the tutorial somehow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e686797-5a2a-47d8-bb01-8ec8db450e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "Collecting pyarrow==7.0\n",
      "  Using cached pyarrow-7.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.8/site-packages (from pyarrow==7.0) (1.19.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "Installing collected packages: pyarrow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xgboost-ray 0.1.4 requires pyarrow<5.0.0, but you have pyarrow 7.0.0 which is incompatible.\n",
      "dominodatalab-data 0.2.1 requires backoff<2.0.0,>=1.11.1, but you have backoff 1.10.0 which is incompatible.\n",
      "dominodatalab-data 0.2.1 requires pyarrow<7.0.0,>=6.0.0, but you have pyarrow 7.0.0 which is incompatible.\u001b[0m\n",
      "Successfully installed pyarrow-7.0.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.8/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow==7.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf4998-5cc6-4c8e-9a92-d9dc60518493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a7a08b-0f1e-4772-9b36-d4e08afbc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "\n",
    "# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION']='python'\n",
    "\n",
    "if ray.is_initialized() == False:\n",
    "    service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "    service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "    ray.init(f\"ray://{service_host}:{service_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9055c06-24b4-485f-b196-a367e7bce516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import modin.pandas as mpd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as pds\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bba7d5de-f014-449d-a0b2-f30162542bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some dummy data in multiple files\n",
    "default_dataset_path = f\"/domino/datasets/local/{os.environ['DOMINO_PROJECT_NAME']}\"\n",
    "\n",
    "def generate_dummy_data_filesplit(n_rows, n_columns, name, n_parts = 10):\n",
    "    dummy_file_root = os.path.join(default_dataset_path, f\"{name}\")\n",
    "    data_columns = {f\"col_{i}\": np.random.standard_normal(n_rows) for i in range(n_columns)}\n",
    "    n_per = n_rows // n_parts\n",
    "    pds.write_dataset(\n",
    "        pa.Table.from_pydict(data_columns),\n",
    "        dummy_file_root,\n",
    "        format='parquet',\n",
    "        max_rows_per_file = n_per,\n",
    "        max_rows_per_group = n_per\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e8fe446-a28e-45be-b0b2-91232e22a3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m [2022-08-08 17:08:29,959 C 19 19] gcs_client.cc:328: Couldn't reconnect to GCS server. The last attempted GCS server address was :0\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m *** StackTrace Information ***\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     ray::SpdLogMessage::Flush()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     ray::RayLog::~RayLog()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     ray::gcs::GcsClient::ReconnectGcsServer()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     std::function<>::operator()()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     ray::rpc::ClientCallImpl<>::OnReplyReceived()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     boost::asio::detail::completion_handler<>::do_complete()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     boost::asio::detail::scheduler::do_run_one()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     boost::asio::detail::scheduler::run()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     boost::asio::io_context::run()\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     main\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m     __libc_start_main\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.66.231)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "generate_dummy_data_filesplit(10**7, 20, \"medium-filesplit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7295a0c-96e5-412d-91db-8ae45769b147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13355247611c4ffd9ffff1e218e9bd9e.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls /domino/datasets/local/Ray-Tutorial/smallest-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b7533f8-1542-447d-86cb-af419db473be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_data_split(n_rows, n_columns, name):\n",
    "    dummy_file_path = os.path.join(default_dataset_path, f\"{name}.parquet\")\n",
    "    arr = np.random.standard_normal((n_rows, n_columns))\n",
    "    df = pd.DataFrame(arr, columns = [str(i) for i in range(n_columns)])\n",
    "    df.to_parquet(dummy_file_path, row_group_size = n_rows // 10)\n",
    "    size_gib = os.path.getsize(dummy_file_path)/(1024*1024*1024)\n",
    "    print(f\"With {n_rows} rows and {n_columns} columns the new file is {size_gib} GiB on disk\")\n",
    "    return dummy_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d519b4a-d25c-4e59-ba09-734d489537b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 10000000 rows and 20 columns the new file is 1.5422909455373883 GiB on disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/domino/datasets/local/Ray-Tutorial/medium-split.parquet'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dummy_data_split(10**7, 20, \"medium-split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9dac16-3167-4816-bf13-966a417a80ba",
   "metadata": {},
   "source": [
    "#### Try with ray data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d9ed457-467d-42a0-8925-16d6de0c95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_file = '/domino/datasets/local/Ray-Tutorial/medium-filesplit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f0f052-28fd-4c59-8ff3-4ac7f9bfc2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_parquet(medium_file, parallelism = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d3b28f-a0e3-489b-a745-4270b37d0376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'col_0': 1.158893411011255, 'col_1': 0.6024888481795232, 'col_2': 0.8547269707463037, 'col_3': -0.7717945711432682, 'col_4': 1.327244959107909, 'col_5': 1.6760976533642786, 'col_6': 0.851013834218383, 'col_7': -0.6105415319538923, 'col_8': -0.8987464297029039, 'col_9': 0.1937053286234583, 'col_10': -0.4011255531356019, 'col_11': -1.142756618209515, 'col_12': -2.0355433205668514, 'col_13': -1.6062197382186973, 'col_14': -1.7626335335372112, 'col_15': -0.5957913560997585, 'col_16': 1.2440529127597844, 'col_17': 0.08486158782383446, 'col_18': -0.9278595131767658, 'col_19': 0.8911579179072887}\n",
      "{'col_0': -1.1585311566375038, 'col_1': 0.155681761397261, 'col_2': -0.10374564341978187, 'col_3': 0.7309110889294081, 'col_4': -0.8441261132098692, 'col_5': -0.4426172541321336, 'col_6': 2.145427727628284, 'col_7': 0.7734108353639534, 'col_8': 0.15244208585701527, 'col_9': 0.5341579368697362, 'col_10': 0.21354196252623878, 'col_11': 0.47904246676231227, 'col_12': 0.03953915050317926, 'col_13': -0.13556525325989377, 'col_14': 0.027108087956649894, 'col_15': -1.38828817629496, 'col_16': -0.02359364095014775, 'col_17': 0.4514799979749801, 'col_18': -0.6220621203730198, 'col_19': -1.6174428235839473}\n",
      "{'col_0': -0.8274336700689704, 'col_1': -0.103442683379517, 'col_2': 0.5326386895996962, 'col_3': -1.0208335822955505, 'col_4': 0.02457744472306014, 'col_5': -2.5150210095705963, 'col_6': 0.09196931793467313, 'col_7': 0.6614094476969256, 'col_8': 0.5660547688867875, 'col_9': 0.28877238208095013, 'col_10': 0.15065016752762012, 'col_11': -0.6064025016952387, 'col_12': 0.7815123898154335, 'col_13': 0.7624828115206352, 'col_14': 0.3447532311292041, 'col_15': 0.04836714331443709, 'col_16': -1.7490090111588246, 'col_17': 1.3175340886702605, 'col_18': 0.12974974153979918, 'col_19': 0.41665238924497133}\n"
     ]
    }
   ],
   "source": [
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d711b23-f16b-42ec-80f6-3d5e1e046682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_transform_batch(t: pa.Table) -> pd.DataFrame:\n",
    "    return t.to_pandas().sum(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75413928-0655-476e-a518-25e065904f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "ds2 = ds.map_batches(dummy_transform_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b77dfc-4548-4049-9798-46b7cef37ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': -1.868768742002446}\n",
      "{'0': -0.6332290800922385}\n",
      "{'0': -0.7050184444842438}\n"
     ]
    }
   ],
   "source": [
    "ds2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49173e2b-0c83-4349-b32c-828fce5c5aa6",
   "metadata": {},
   "source": [
    "### Final Notes\n",
    "Yesss, victory!!!!\n",
    "\n",
    "Must clean this up later and summarize for Dan and Ben, etc. Don't forget:\n",
    "* Needed pyarrow 7.0 to get the convenient option for max_rows_per_file\n",
    "* `ray.data.read_parquet` needs `parallelism` set to be smart - it fails if you do not set this even on the split files (prove that again to be sure). But it also fails if you try to set it on the monolith file because of the absolute money quote in the docs \"Parallelism may be limited by the number of files of the dataset.\" (From https://docs.ray.io/en/latest/data/package-ref.html)\n",
    "* Note the Ray Web UI stuff closer - it does seem when parallelism is enabled ray data is somewhat lazy. The ds.show did not seem to trigger a full read, needed my dummy sum operation to do that. Should I make the modin example do something similar to make sure it is truly smart enough to do similar things on the monolith file? (Maybe this is getting into more advanced details-of-intermediate-tutorial areas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f0569-8000-4a6b-9932-f86a7dbf441c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
